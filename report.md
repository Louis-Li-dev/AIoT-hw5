# AI æ–‡æœ¬æª¢æ¸¬å™¨ç¨‹å¼ç¢¼é€è¡Œè§£é‡‹

## ğŸ“¦ åŒ¯å…¥æ¨¡çµ„å€æ®µï¼ˆç¬¬ 1-23 è¡Œï¼‰

### ç¬¬ 1-5 è¡Œï¼šæª”æ¡ˆèªªæ˜æ–‡ä»¶

```python
"""
Streamlit app: AI vs Human text detector with multilingual support.
Uses Hugging Face text-classification models with multiple fallback options.
"""
```

**èªªæ˜ï¼š** é€™æ˜¯ç¨‹å¼çš„æ–‡æª”å­—ä¸²ï¼Œæè¿°é€™æ˜¯ä¸€å€‹ä½¿ç”¨ Hugging Face æ¨¡å‹çš„ AI/äººé¡æ–‡æœ¬æª¢æ¸¬å™¨ï¼Œæ”¯æ´å¤šèªè¨€å’Œå¤šå€‹å‚™ç”¨é¸é …ã€‚

---

### ç¬¬ 7 è¡Œï¼šå¼•å…¥æœªä¾†ç‰¹æ€§

```python
from __future__ import annotations
```

**èªªæ˜ï¼š** å•Ÿç”¨å»¶é²é¡å‹è¨»è§£è©•ä¼°ï¼Œè®“æˆ‘å€‘å¯ä»¥åœ¨é¡å‹æç¤ºä¸­ä½¿ç”¨å­—ä¸²å½¢å¼çš„é¡å‹ï¼Œæé«˜ç¨‹å¼ç¢¼çš„ç›¸å®¹æ€§ã€‚

---

### ç¬¬ 9-10 è¡Œï¼šåŸºç¤æ¨¡çµ„

```python
import os
import sys
```

**èªªæ˜ï¼š**

* `os`ï¼šç”¨æ–¼å­˜å–ä½œæ¥­ç³»çµ±åŠŸèƒ½ï¼Œç‰¹åˆ¥æ˜¯ç’°å¢ƒè®Šæ•¸
* `sys`ï¼šç”¨æ–¼å­˜å–ç³»çµ±ç‰¹å®šçš„åƒæ•¸å’Œå‡½æ•¸

---

### ç¬¬ 12-13 è¡Œï¼šé—œéµä¿®å¾©

```python
# CRITICAL: Set this BEFORE importing streamlit to avoid torch watcher issues
os.environ["STREAMLIT_WATCHER_TYPE"] = "none"
```

**èªªæ˜ï¼š****é€™æ˜¯æœ€é‡è¦çš„ä¸€è¡Œï¼** åœ¨è¼‰å…¥ Streamlit ä¹‹å‰ï¼Œå°‡æª”æ¡ˆç›£è¦–å™¨é¡å‹è¨­ç‚º "none"ï¼Œé¿å… Streamlit èˆ‡ PyTorch å…§éƒ¨çµæ§‹ç”¢ç”Ÿè¡çªã€‚é€™è§£æ±ºäº†ä½ é‡åˆ°çš„ `torch.classes` éŒ¯èª¤ã€‚

---

### ç¬¬ 15 è¡Œï¼šè¼‰å…¥ Streamlit

```python
import streamlit as st
```

**èªªæ˜ï¼š** è¼‰å…¥ Streamlit æ¡†æ¶ï¼Œé€™æ˜¯å»ºç«‹ç¶²é æ‡‰ç”¨ç¨‹å¼çš„ä¸»è¦å·¥å…·ã€‚**å¿…é ˆåœ¨è¨­å®šç’°å¢ƒè®Šæ•¸ä¹‹å¾Œæ‰èƒ½è¼‰å…¥ã€‚**

---

### ç¬¬ 16 è¡Œï¼šå‹åˆ¥æç¤º

```python
from typing import Tuple
```

**èªªæ˜ï¼š** å¾ `typing` æ¨¡çµ„è¼‰å…¥ `Tuple` é¡å‹ï¼Œç”¨æ–¼å‡½æ•¸å›å‚³å€¼çš„å‹åˆ¥è¨»è§£ã€‚

---

### ç¬¬ 18-25 è¡Œï¼šæ¢ä»¶å¼åŒ¯å…¥ Transformers

```python
try:
    from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
except Exception as exc:
    AutoTokenizer = None  # type: ignore
    AutoModelForSequenceClassification = None  # type: ignore
    pipeline = None  # type: ignore
    _IMPORT_ERROR = exc
else:
    _IMPORT_ERROR = None
```

**èªªæ˜ï¼š**

* å˜—è©¦å¾ `transformers` å¥—ä»¶è¼‰å…¥ä¸‰å€‹é—œéµå…ƒä»¶ï¼š
  * `AutoTokenizer`ï¼šè‡ªå‹•é¸æ“‡é©ç•¶çš„æ–‡æœ¬åˆ†è©å™¨
  * `AutoModelForSequenceClassification`ï¼šç”¨æ–¼åºåˆ—åˆ†é¡çš„æ¨¡å‹
  * `pipeline`ï¼šç°¡åŒ–æ¨¡å‹ä½¿ç”¨çš„ç®¡é“å·¥å…·
* å¦‚æœè¼‰å…¥å¤±æ•—ï¼ˆä¾‹å¦‚æœªå®‰è£å¥—ä»¶ï¼‰ï¼Œå°‡é€™äº›è®Šæ•¸è¨­ç‚º `None` ä¸¦è¨˜éŒ„éŒ¯èª¤
* é€™ç¨®è¨­è¨ˆè®“ç¨‹å¼å³ä½¿æ²’æœ‰å®‰è£ AI æ¨¡å‹ä¹Ÿèƒ½é‹è¡Œï¼ˆä½¿ç”¨å•Ÿç™¼å¼æ–¹æ³•ï¼‰

---

## ğŸ”§ é…ç½®è¨­å®šå€æ®µï¼ˆç¬¬ 28-50 è¡Œï¼‰

### ç¬¬ 28-36 è¡Œï¼šå¯ç”¨æ¨¡å‹å­—å…¸

```python
AVAILABLE_MODELS = {
    "fakespot-ai/roberta-base-ai-text-detection-v1": {
        "name": "Fakespot AI Detector",
        "lang": "English",
        "description": "Modern AI text detection model"
    },
}
```

**èªªæ˜ï¼š**

* å®šç¾©ä¸€å€‹å­—å…¸ï¼Œå­˜å„²å¯ç”¨çš„ AI æª¢æ¸¬æ¨¡å‹
* éµï¼šHugging Face ä¸Šçš„æ¨¡å‹è­˜åˆ¥ç¢¼
* å€¼ï¼šåŒ…å«æ¨¡å‹åç¨±ã€æ”¯æ´èªè¨€ã€æè¿°çš„å­å­—å…¸
* ç›®å‰åªæœ‰ä¸€å€‹æ¨¡å‹ï¼ˆä½ å¯ä»¥æ–°å¢æ›´å¤šï¼‰

---

### ç¬¬ 38 è¡Œï¼šé è¨­æ¨¡å‹

```python
DEFAULT_MODEL = "Hello-SimpleAI/chatgpt-detector-roberta"
```

**èªªæ˜ï¼š** å®šç¾©é è¨­ä½¿ç”¨çš„æ¨¡å‹ï¼ˆé›–ç„¶é€™å€‹æ¨¡å‹ä¸åœ¨ `AVAILABLE_MODELS` ä¸­ï¼Œå¯èƒ½æ˜¯ç¨‹å¼ç¢¼çš„éºç•™éƒ¨åˆ†ï¼‰ã€‚

---

### ç¬¬ 40-49 è¡Œï¼šæ¨™ç±¤å°æ‡‰å­—å…¸

```python
LABEL_TO_CLASS = {
    "LABEL_0": "human",
    "LABEL_1": "ai",
    "human": "human",
    "ai": "ai",
    "fake": "ai",
    "real": "human",
    "0": "human",
    "1": "ai",
}
```

**èªªæ˜ï¼š**

* å»ºç«‹æ¨™ç±¤æ˜ å°„è¡¨ï¼Œå°‡ä¸åŒæ¨¡å‹è¼¸å‡ºçš„æ¨™ç±¤çµ±ä¸€è½‰æ›ç‚º "human" æˆ– "ai"
* ä¸åŒæ¨¡å‹å¯èƒ½ä½¿ç”¨ä¸åŒçš„æ¨™ç±¤æ ¼å¼ï¼ˆLABEL\_0ã€0ã€humanã€real ç­‰ï¼‰
* é€™å€‹å­—å…¸ç¢ºä¿æ‰€æœ‰æ ¼å¼éƒ½èƒ½æ­£ç¢ºè§£è®€

---

## ğŸ§  æ¨¡å‹è¼‰å…¥å‡½æ•¸ï¼ˆç¬¬ 52-75 è¡Œï¼‰

### ç¬¬ 52-53 è¡Œï¼šå‡½æ•¸å®šç¾©èˆ‡è£é£¾å™¨

```python
@st.cache_resource(show_spinner=True)
def load_detector(model_name: str):
```

**èªªæ˜ï¼š**

* `@st.cache_resource`ï¼šStreamlit è£é£¾å™¨ï¼Œå°‡æ¨¡å‹å¿«å–åœ¨è¨˜æ†¶é«”ä¸­ï¼Œé¿å…é‡è¤‡è¼‰å…¥
* `show_spinner=True`ï¼šè¼‰å…¥æ™‚é¡¯ç¤ºæ—‹è½‰åœ–ç¤º
* å‡½æ•¸æ¥æ”¶ `model_name`ï¼ˆå­—ä¸²ï¼‰ä½œç‚ºåƒæ•¸

---

### ç¬¬ 54 è¡Œï¼šæ–‡æª”å­—ä¸²

```python
    """Load the Hugging Face classifier. Falls back to None if unavailable."""
```

**èªªæ˜ï¼š** å‡½æ•¸èªªæ˜ï¼šè¼‰å…¥ Hugging Face åˆ†é¡å™¨ï¼Œå¦‚æœç„¡æ³•è¼‰å…¥å‰‡è¿”å› Noneã€‚

---

### ç¬¬ 55-56 è¡Œï¼šæª¢æŸ¥ Transformers åŒ¯å…¥

```python
    if _IMPORT_ERROR is not None:
        return None, f"Import error: {_IMPORT_ERROR}"
```

**èªªæ˜ï¼š** å¦‚æœä¹‹å‰è¼‰å…¥ `transformers` æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œç›´æ¥è¿”å› None å’ŒéŒ¯èª¤è¨Šæ¯ï¼Œä¸å˜—è©¦è¼‰å…¥æ¨¡å‹ã€‚

---

### ç¬¬ 57-59 è¡Œï¼šæª¢æŸ¥ PyTorch

```python
    try:
        import torch
    except Exception as exc:
        return None, f"Torch import error: {exc}"
```

**èªªæ˜ï¼š**

* å˜—è©¦è¼‰å…¥ PyTorchï¼ˆæ·±åº¦å­¸ç¿’æ¡†æ¶ï¼‰
* é€™è£¡ä½¿ç”¨å»¶é²è¼‰å…¥ï¼ˆlazy importï¼‰ï¼Œé¿å…å•Ÿå‹•æ™‚çš„å•é¡Œ
* å¦‚æœè¼‰å…¥å¤±æ•—ï¼Œè¿”å›éŒ¯èª¤è¨Šæ¯

---

### ç¬¬ 61-75 è¡Œï¼šè¼‰å…¥æ¨¡å‹ä¸»é«”

```python
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSequenceClassification.from_pretrained(model_name)
        device = 0 if torch and torch.cuda.is_available() else -1
        clf = pipeline(
            task="text-classification",
            model=model,
            tokenizer=tokenizer,
            device=device,
            top_k=None,
        )
        return clf, None
    except Exception as exc:
        return None, str(exc)
```

**èªªæ˜ï¼š**

* **ç¬¬ 62 è¡Œï¼š** å¾ Hugging Face è¼‰å…¥åˆ†è©å™¨
* **ç¬¬ 63 è¡Œï¼š** å¾ Hugging Face è¼‰å…¥é è¨“ç·´æ¨¡å‹
* **ç¬¬ 64 è¡Œï¼š** æ±ºå®šä½¿ç”¨ GPUï¼ˆdevice=0ï¼‰æˆ– CPUï¼ˆdevice=-1ï¼‰
  * å¦‚æœæœ‰ CUDA GPU å¯ç”¨ï¼Œä½¿ç”¨ GPU åŠ é€Ÿ
* **ç¬¬ 65-70 è¡Œï¼š** å»ºç«‹ pipelineï¼ˆç®¡é“ï¼‰ç‰©ä»¶
  * `task="text-classification"`ï¼šæŒ‡å®šä»»å‹™ç‚ºæ–‡æœ¬åˆ†é¡
  * `top_k=None`ï¼šè¿”å›æ‰€æœ‰é¡åˆ¥çš„æ©Ÿç‡
* **ç¬¬ 71 è¡Œï¼š** æˆåŠŸæ™‚è¿”å› pipeline å’Œ Noneï¼ˆç„¡éŒ¯èª¤ï¼‰
* **ç¬¬ 73 è¡Œï¼š** å¤±æ•—æ™‚è¿”å› None å’ŒéŒ¯èª¤è¨Šæ¯

---

## ğŸ¯ æ¨¡å‹é æ¸¬å‡½æ•¸ï¼ˆç¬¬ 78-103 è¡Œï¼‰

### ç¬¬ 78-79 è¡Œï¼šå‡½æ•¸å®šç¾©

```python
def predict_with_model(clf, text: str) -> Tuple[float, float]:
    """Return (ai_prob, human_prob) using the HF pipeline."""
```

**èªªæ˜ï¼š**

* æ¥æ”¶ pipeline ç‰©ä»¶å’Œæ–‡æœ¬å­—ä¸²
* è¿”å›ä¸€å€‹åŒ…å«å…©å€‹æµ®é»æ•¸çš„å…ƒçµ„ï¼š(AI æ©Ÿç‡, äººé¡æ©Ÿç‡)

---

### ç¬¬ 80 è¡Œï¼šåŸ·è¡Œæ¨è«–

```python
    outputs = clf(text, truncation=True, max_length=512)
```

**èªªæ˜ï¼š**

* ä½¿ç”¨ pipeline å°æ–‡æœ¬é€²è¡Œåˆ†é¡
* `truncation=True`ï¼šå¦‚æœæ–‡æœ¬å¤ªé•·å‰‡æˆªæ–·
* `max_length=512`ï¼šæœ€å¤§è™•ç† 512 å€‹ tokenï¼ˆç´„ 300-400 å€‹è‹±æ–‡å–®å­—ï¼‰

---

### ç¬¬ 82-89 è¡Œï¼šè™•ç†è¼¸å‡ºæ ¼å¼

```python
    # Handle different output formats
    if isinstance(outputs, list) and outputs:
        if isinstance(outputs[0], dict) and "label" in outputs[0]:
            # Single prediction returned as list of dicts
            outputs = [outputs]
        elif isinstance(outputs[0], list):
            # Already in correct format
            pass
```

**èªªæ˜ï¼š**

* ä¸åŒæ¨¡å‹å’Œé…ç½®å¯èƒ½è¿”å›ä¸åŒæ ¼å¼çš„è¼¸å‡º
* é€™æ®µç¨‹å¼ç¢¼çµ±ä¸€è¼¸å‡ºæ ¼å¼ç‚º `[[{label: ..., score: ...}, ...]]`
* **ç¬¬ 84-86 è¡Œï¼š** å¦‚æœæ˜¯å–®ä¸€é æ¸¬è¿”å›ç‚ºå­—å…¸åˆ—è¡¨ï¼Œå°‡å…¶åŒ…è£æˆäºŒç¶­åˆ—è¡¨
* **ç¬¬ 87-89 è¡Œï¼š** å¦‚æœå·²ç¶“æ˜¯æ­£ç¢ºæ ¼å¼ï¼Œä¸åšä»»ä½•è™•ç†

---

### ç¬¬ 91-92 è¡Œï¼šæå–æœ€ä½³é æ¸¬

```python
    # Get the best prediction
    best = outputs[0][0] if isinstance(outputs[0], list) else outputs[0]
    label_str = str(best.get("label", "")).lower()
```

**èªªæ˜ï¼š**

* ç²å–ä¿¡å¿ƒåº¦æœ€é«˜çš„é æ¸¬çµæœ
* å°‡æ¨™ç±¤è½‰æ›ç‚ºå°å¯«å­—ä¸²ï¼Œæ–¹ä¾¿å¾ŒçºŒè™•ç†

---

### ç¬¬ 94-96 è¡Œï¼šæ¨™ç±¤æ¨™æº–åŒ–

```python
    # Normalize label to "ai" or "human"
    label = LABEL_TO_CLASS.get(label_str, "ai")
    score = float(best.get("score", 0.5))
```

**èªªæ˜ï¼š**

* ä½¿ç”¨ä¹‹å‰å®šç¾©çš„ `LABEL_TO_CLASS` å­—å…¸å°‡æ¨™ç±¤è½‰æ›ç‚º "ai" æˆ– "human"
* æå–ä¿¡å¿ƒåº¦åˆ†æ•¸ï¼ˆé è¨­ 0.5 è¡¨ç¤ºä¸ç¢ºå®šï¼‰

---

### ç¬¬ 98-103 è¡Œï¼šè¨ˆç®—æ©Ÿç‡

```python
    # Calculate AI probability
    if label == "ai":
        ai_prob = score
    else:
        ai_prob = 1 - score
  
    ai_prob = min(max(ai_prob, 0.0), 1.0)
    return ai_prob, 1 - ai_prob
```

**èªªæ˜ï¼š**

* å¦‚æœé æ¸¬ç‚º AIï¼Œåˆ†æ•¸å°±æ˜¯ AI æ©Ÿç‡
* å¦‚æœé æ¸¬ç‚ºäººé¡ï¼ŒAI æ©Ÿç‡ = 1 - åˆ†æ•¸
* ç¢ºä¿æ©Ÿç‡åœ¨ 0.0 åˆ° 1.0 ä¹‹é–“ï¼ˆä½¿ç”¨ `min` å’Œ `max` é™åˆ¶ç¯„åœï¼‰
* è¿”å› (AI æ©Ÿç‡, äººé¡æ©Ÿç‡)

---

## ğŸ“Š å•Ÿç™¼å¼å‚™ç”¨æ–¹æ¡ˆï¼ˆç¬¬ 106-125 è¡Œï¼‰

### ç¬¬ 106-110 è¡Œï¼šå‡½æ•¸å®šç¾©èˆ‡åŸºç¤æª¢æŸ¥

```python
def fallback_heuristic(text: str) -> Tuple[float, float]:
    """
    Lightweight heuristic: mixes length, repetition, and punctuation richness.
    Returns (ai_prob, human_prob).
    """
    stripped = text.strip()
    if not stripped:
        return 0.5, 0.5
```

**èªªæ˜ï¼š**

* ç•¶æ¨¡å‹ç„¡æ³•è¼‰å…¥æ™‚ä½¿ç”¨çš„ç°¡å–®è¦å‰‡åˆ¤æ–·æ³•
* å¦‚æœæ–‡æœ¬ç‚ºç©ºï¼Œè¿”å› 50% æ©Ÿç‡ï¼ˆä¸ç¢ºå®šï¼‰

---

### ç¬¬ 112-115 è¡Œï¼šè¨ˆç®—æ–‡æœ¬ç‰¹å¾µ

```python
    length = len(stripped)
    unique_ratio = len(set(stripped)) / max(length, 1)
    punctuation = sum(ch in "ï¼Œã€‚ã€ï¼ï¼Ÿ,.?!ï¼›;ï¼š" for ch in stripped) / max(length, 1)
    digit_ratio = sum(ch.isdigit() for ch in stripped) / max(length, 1)
```

**èªªæ˜ï¼š**

* **ç¬¬ 112 è¡Œï¼š** æ–‡æœ¬é•·åº¦
* **ç¬¬ 113 è¡Œï¼š** ç¨ç‰¹å­—å…ƒæ¯”ä¾‹ï¼ˆå­—å…ƒç¨®é¡ Ã· ç¸½å­—å…ƒæ•¸ï¼‰
  * AI æ–‡æœ¬é€šå¸¸é‡è¤‡æ€§è¼ƒé«˜ï¼Œé€™å€‹æ¯”ä¾‹è¼ƒä½
* **ç¬¬ 114 è¡Œï¼š** æ¨™é»ç¬¦è™Ÿå¯†åº¦ï¼ˆæ”¯æ´ä¸­è‹±æ–‡æ¨™é»ï¼‰
  * AI æ–‡æœ¬æ¨™é»ä½¿ç”¨å¯èƒ½è¼ƒå–®èª¿
* **ç¬¬ 115 è¡Œï¼š** æ•¸å­—ä½”æ¯”
  * æŸäº› AI ç”Ÿæˆæ–‡æœ¬å¯èƒ½åŒ…å«è¼ƒå¤šæ•¸å­—

---

### ç¬¬ 117-122 è¡Œï¼šè¨ˆç®— AI åˆ†æ•¸

```python
    # AI text tends to be more uniform and have less punctuation variety
    ai_score = (
        0.35 * (1 - unique_ratio)
        + 0.35 * max(0.0, 0.15 - punctuation)
        + 0.3 * min(0.2, digit_ratio) * 5
    )
```

**èªªæ˜ï¼š**

* ä½¿ç”¨åŠ æ¬Šçµ„åˆè¨ˆç®— AI åˆ†æ•¸ï¼š
  * **35% æ¬Šé‡ï¼š** ä½ç¨ç‰¹æ€§ï¼ˆ1 - unique\_ratioï¼‰
  * **35% æ¬Šé‡ï¼š** ä½æ¨™é»å¯†åº¦ï¼ˆæ¨™é» < 15% æ™‚è¨ˆåˆ†ï¼‰
  * **30% æ¬Šé‡ï¼š** æ•¸å­—æ¯”ä¾‹ï¼ˆæœ€å¤šè¨ˆç®—åˆ° 20%ï¼‰
* é€™äº›ç‰¹å¾µæ˜¯åŸºæ–¼ç¶“é©—è§€å¯Ÿï¼Œå¯¦éš›æ•ˆæœæœ‰é™

---

### ç¬¬ 123-124 è¡Œï¼šè¿”å›çµæœ

```python
    ai_prob = min(max(ai_score, 0.0), 1.0)
    return ai_prob, 1 - ai_prob
```

**èªªæ˜ï¼š**

* ç¢ºä¿ AI æ©Ÿç‡åœ¨ 0.0 åˆ° 1.0 ä¹‹é–“
* è¿”å› (AI æ©Ÿç‡, äººé¡æ©Ÿç‡)

---

## ğŸ–¥ï¸ ä¸»ç¨‹å¼ä»‹é¢ï¼ˆç¬¬ 127-228 è¡Œï¼‰

### ç¬¬ 127-130 è¡Œï¼šé é¢é…ç½®

```python
def main():
    st.set_page_config(page_title="AI / Human æ–‡ç« åµæ¸¬å™¨", page_icon="ğŸ¤–", layout="wide")
    st.title("ğŸ¤– AI / Human æ–‡ç« åµæ¸¬å™¨")
    st.write("è¼¸å…¥ä»»æ„æ–‡æœ¬ï¼Œç«‹å³åˆ¤æ–·æ˜¯ AI é‚„æ˜¯äººé¡æ’°å¯«ã€‚")
```

**èªªæ˜ï¼š**

* è¨­å®šç¶²é æ¨™é¡Œã€åœ–ç¤ºå’Œå¯¬ç‰ˆé¢é…ç½®
* é¡¯ç¤ºæ‡‰ç”¨ç¨‹å¼æ¨™é¡Œå’Œèªªæ˜æ–‡å­—

---

### ç¬¬ 132-142 è¡Œï¼šå´é‚Šæ¬„æ¨¡å‹é¸æ“‡

```python
    with st.sidebar:
        st.subheader("âš™ï¸ è¨­å®š")
      
        # Model selection
        model_choice = st.selectbox(
            "é¸æ“‡æ¨¡å‹",
            options=list(AVAILABLE_MODELS.keys()),
            format_func=lambda x: AVAILABLE_MODELS[x]["name"],
            index=0
        )
```

**èªªæ˜ï¼š**

* åœ¨å´é‚Šæ¬„å»ºç«‹ä¸‹æ‹‰é¸å–®è®“ä½¿ç”¨è€…é¸æ“‡æ¨¡å‹
* `options`ï¼šæ¨¡å‹è­˜åˆ¥ç¢¼åˆ—è¡¨
* `format_func`ï¼šé¡¯ç¤ºå‹å–„çš„æ¨¡å‹åç¨±è€Œéè­˜åˆ¥ç¢¼
* `index=0`ï¼šé è¨­é¸æ“‡ç¬¬ä¸€å€‹æ¨¡å‹

---

### ç¬¬ 144-145 è¡Œï¼šé¡¯ç¤ºæ¨¡å‹è³‡è¨Š

```python
        st.caption(f"èªè¨€: {AVAILABLE_MODELS[model_choice]['lang']}")
        st.caption(f"{AVAILABLE_MODELS[model_choice]['description']}")
```

**èªªæ˜ï¼š** é¡¯ç¤ºæ‰€é¸æ¨¡å‹æ”¯æ´çš„èªè¨€å’Œæè¿°ã€‚

---

### ç¬¬ 147-149 è¡Œï¼šåˆ†éš”ç·šèˆ‡ç‹€æ…‹å€å¡Š

```python
        st.markdown("---")
        st.subheader("ğŸ“Š æ¨¡å‹ç‹€æ…‹")
      
        clf, load_err = load_detector(model_choice)
```

**èªªæ˜ï¼š**

* æ–°å¢æ°´å¹³åˆ†éš”ç·š
* å‘¼å« `load_detector` è¼‰å…¥æ¨¡å‹ï¼ˆç”±æ–¼æœ‰ `@st.cache_resource`ï¼Œåªæœƒè¼‰å…¥ä¸€æ¬¡ï¼‰

---

### ç¬¬ 151-161 è¡Œï¼šé¡¯ç¤ºè¼‰å…¥ç‹€æ…‹

```python
        if clf:
            st.success(f"âœ… æ¨¡å‹å·²è¼‰å…¥")
            st.caption(f"ä½¿ç”¨æ¨¡å‹: {AVAILABLE_MODELS[model_choice]['name']}")
        else:
            st.warning("âš ï¸ ä½¿ç”¨ç°¡æ˜“å•Ÿç™¼å¼åµæ¸¬")
            if load_err:
                with st.expander("æŸ¥çœ‹éŒ¯èª¤è©³æƒ…"):
                    st.code(load_err, language="text")
            elif _IMPORT_ERROR:
                with st.expander("æŸ¥çœ‹åŒ¯å…¥éŒ¯èª¤"):
                    st.code(str(_IMPORT_ERROR), language="text")
```

**èªªæ˜ï¼š**

* å¦‚æœæ¨¡å‹è¼‰å…¥æˆåŠŸï¼Œé¡¯ç¤ºç¶ è‰²æˆåŠŸè¨Šæ¯
* å¦‚æœè¼‰å…¥å¤±æ•—ï¼Œé¡¯ç¤ºé»ƒè‰²è­¦å‘Šä¸¦æä¾›å¯å±•é–‹çš„éŒ¯èª¤è©³æƒ…
* è®“ä½¿ç”¨è€…äº†è§£ç•¶å‰ä½¿ç”¨çš„æ˜¯æ¨¡å‹é‚„æ˜¯å•Ÿç™¼å¼æ–¹æ³•

---

### ç¬¬ 163-178 è¡Œï¼šä½¿ç”¨æç¤º

```python
        st.markdown("---")
        st.subheader("ğŸ’¡ ä½¿ç”¨æç¤º")
        st.markdown("""
        **å®‰è£ä¾è³´ï¼š**
        ```bash
        pip install streamlit transformers torch
        ```
      
        **æ³¨æ„äº‹é …ï¼š**
        - è‹±æ–‡æ–‡æœ¬æ•ˆæœæœ€ä½³
        - ä¸­æ–‡éœ€ä½¿ç”¨å•Ÿç™¼å¼æ–¹æ³•
        - é¦–æ¬¡è¼‰å…¥éœ€ä¸‹è¼‰æ¨¡å‹ï¼ˆç´„ 500MBï¼‰
        - GPU å¯åŠ é€Ÿæ¨è«–
        """)
```

**èªªæ˜ï¼š**

* åœ¨å´é‚Šæ¬„é¡¯ç¤ºå®‰è£èªªæ˜å’Œä½¿ç”¨æ³¨æ„äº‹é …
* ä½¿ç”¨ Markdown æ ¼å¼åŒ–æ–‡å­—ï¼ŒåŒ…å«ç¨‹å¼ç¢¼å€å¡Š

---

### ç¬¬ 180-187 è¡Œï¼šæ–‡æœ¬è¼¸å…¥å€

```python
    # Main content area
    default_text = "This is a sample text to test the AI/Human detector. The quick brown fox jumps over the lazy dog."
    text = st.text_area(
        "è¼¸å…¥æ–‡æœ¬ (å»ºè­°ä½¿ç”¨è‹±æ–‡ä»¥ç²å¾—æœ€ä½³æ•ˆæœ)",
        value=default_text,
        height=220,
        help="è¼¸å…¥æ‚¨æƒ³è¦æª¢æ¸¬çš„æ–‡æœ¬"
    )
```

**èªªæ˜ï¼š**

* å»ºç«‹å¤šè¡Œæ–‡æœ¬è¼¸å…¥æ¡†
* æä¾›é è¨­ç¯„ä¾‹æ–‡æœ¬
* é«˜åº¦è¨­ç‚º 220 åƒç´ 
* æä¾›æç¤ºè¨Šæ¯

---

### ç¬¬ 189-196 è¡Œï¼šåŸ·è¡Œæª¢æ¸¬

```python
    if text.strip():
        with st.spinner("åˆ†æä¸­..."):
            if clf:
                ai_prob, human_prob = predict_with_model(clf, text)
                method_used = f"ğŸ”¬ {AVAILABLE_MODELS[model_choice]['name']}"
            else:
                ai_prob, human_prob = fallback_heuristic(text)
                method_used = "ğŸ“ å•Ÿç™¼å¼åµæ¸¬"
```

**èªªæ˜ï¼š**

* åªåœ¨æœ‰è¼¸å…¥æ–‡æœ¬æ™‚æ‰åŸ·è¡Œæª¢æ¸¬
* é¡¯ç¤ºæ—‹è½‰è¼‰å…¥å‹•ç•«
* å¦‚æœæœ‰æ¨¡å‹ï¼Œä½¿ç”¨æ¨¡å‹é æ¸¬ï¼›å¦å‰‡ä½¿ç”¨å•Ÿç™¼å¼æ–¹æ³•
* è¨˜éŒ„ä½¿ç”¨çš„æ–¹æ³•ä»¥ä¾¿é¡¯ç¤º

---

### ç¬¬ 198-212 è¡Œï¼šé¡¯ç¤ºçµæœ

```python
        # Display results
        st.markdown("### ğŸ“ˆ æª¢æ¸¬çµæœ")
      
        col1, col2 = st.columns(2)
        with col1:
            st.metric(
                "ğŸ¤– AI æ©Ÿç‡",
                f"{ai_prob * 100:.1f}%",
                delta=f"{(ai_prob - 0.5) * 100:+.1f}%" if ai_prob != 0.5 else None
            )
            st.progress(min(ai_prob, 1.0))
          
        with col2:
            st.metric(
                "ğŸ‘¤ Human æ©Ÿç‡",
                f"{human_prob * 100:.1f}%",
                delta=f"{(human_prob - 0.5) * 100:+.1f}%" if human_prob != 0.5 else None
            )
            st.progress(min(human_prob, 1.0))
```

**èªªæ˜ï¼š**

* å»ºç«‹å…©æ¬„å¸ƒå±€é¡¯ç¤ºçµæœ
* **å·¦æ¬„ï¼š** AI æ©Ÿç‡ï¼Œä½¿ç”¨ `st.metric` é¡¯ç¤ºæ•¸å€¼å’Œèˆ‡ 50% çš„å·®ç•°
* **å³æ¬„ï¼š** äººé¡æ©Ÿç‡
* å…©æ¬„éƒ½åŒ…å«é€²åº¦æ¢è¦–è¦ºåŒ–
* `delta` åƒæ•¸é¡¯ç¤ºåé›¢ 50% çš„ç¨‹åº¦ï¼ˆæ­£å€¼é¡¯ç¤ºç¶ è‰²ä¸Šç®­é ­ï¼Œè² å€¼é¡¯ç¤ºç´…è‰²ä¸‹ç®­é ­ï¼‰

---

### ç¬¬ 214-225 è¡Œï¼šçµæœè©®é‡‹

```python
        # Interpretation
        st.markdown("---")
        if ai_prob > 0.75:
            st.error("ğŸ¤– **å¾ˆå¯èƒ½æ˜¯ AI ç”Ÿæˆçš„æ–‡æœ¬**")
        elif ai_prob > 0.6:
            st.warning("âš ï¸ **å¯èƒ½æ˜¯ AI ç”Ÿæˆçš„æ–‡æœ¬**")
        elif ai_prob > 0.4:
            st.info("ğŸ¤” **ç„¡æ³•ç¢ºå®šä¾†æº**")
        elif ai_prob > 0.25:
            st.warning("âš ï¸ **å¯èƒ½æ˜¯äººé¡æ’°å¯«**")
        else:
            st.success("ğŸ‘¤ **å¾ˆå¯èƒ½æ˜¯äººé¡æ’°å¯«**")
      
        st.caption(f"æ¨æ–·æ–¹å¼ï¼š{method_used}")
        st.caption(f"æ–‡æœ¬é•·åº¦ï¼š{len(text)} å­—å…ƒ")
```

**èªªæ˜ï¼š**

* æ ¹æ“š AI æ©Ÿç‡çµ¦å‡ºè§£è®€ï¼š
  * > 75%ï¼šå¾ˆå¯èƒ½æ˜¯ AIï¼ˆç´…è‰²éŒ¯èª¤è¨Šæ¯ï¼‰
    >
  * 60-75%ï¼šå¯èƒ½æ˜¯ AIï¼ˆé»ƒè‰²è­¦å‘Šï¼‰
  * 40-60%ï¼šä¸ç¢ºå®šï¼ˆè—è‰²è³‡è¨Šï¼‰
  * 25-40%ï¼šå¯èƒ½æ˜¯äººé¡ï¼ˆé»ƒè‰²è­¦å‘Šï¼‰
  * < 25%ï¼šå¾ˆå¯èƒ½æ˜¯äººé¡ï¼ˆç¶ è‰²æˆåŠŸè¨Šæ¯ï¼‰
* é¡¯ç¤ºä½¿ç”¨çš„æª¢æ¸¬æ–¹æ³•å’Œæ–‡æœ¬é•·åº¦

---

### ç¬¬ 227-228 è¡Œï¼šç©ºæ–‡æœ¬æç¤º

```python
    else:
        st.info("ğŸ‘† è«‹åœ¨ä¸Šæ–¹è¼¸å…¥æ–‡æœ¬ä»¥é€²è¡Œåµæ¸¬")
```

**èªªæ˜ï¼š** å¦‚æœæ²’æœ‰è¼¸å…¥æ–‡æœ¬ï¼Œé¡¯ç¤ºæç¤ºè¨Šæ¯ã€‚

---

## ğŸš€ ç¨‹å¼é€²å…¥é»ï¼ˆç¬¬ 231-232 è¡Œï¼‰

### ç¬¬ 231-232 è¡Œï¼šåŸ·è¡Œä¸»ç¨‹å¼

```python
if __name__ == "__main__":
    main()
```

**èªªæ˜ï¼š**

* é€™æ˜¯ Python çš„æ¨™æº–é€²å…¥é»å¯«æ³•
* ç•¶ç›´æ¥åŸ·è¡Œæ­¤æª”æ¡ˆæ™‚ï¼Œå‘¼å« `main()` å‡½æ•¸å•Ÿå‹•æ‡‰ç”¨ç¨‹å¼
* å¦‚æœæ­¤æª”æ¡ˆè¢«å…¶ä»–ç¨‹å¼åŒ¯å…¥ï¼Œå‰‡ä¸æœƒè‡ªå‹•åŸ·è¡Œ

---

### ç¬¬ 233-245 è¡Œï¼šè¨»è§£æ‰çš„ç¨ç«‹æ¸¬è©¦ç¨‹å¼ç¢¼

```python
    # if "streamlit" in sys.argv[0]:
    #     main()
    # else:
    #     # Allow running as a script for quick checks
    #     sample = "The weather is nice today, let's go for a walk in the park."
    #     print(f"Sample input: {sample}")
    #     clf, err = load_detector(DEFAULT_MODEL)
    #     if clf:
    #         ai_prob, human_prob = predict_with_model(clf, sample)
    #         print(f"AI: {ai_prob:.2%}, Human: {human_prob:.2%} (model)")
    #     else:
    #         ai_prob, human_prob = fallback_heuristic(sample)
    #         print(f"AI: {ai_prob:.2%}, Human: {human_prob:.2%} (heuristic)")
    #         if err:
    #             print(f"Error: {err}")
```

**èªªæ˜ï¼š**

* é€™æ˜¯è¢«è¨»è§£æ‰çš„æ›¿ä»£é€²å…¥é»é‚è¼¯
* åŸæœ¬è¨­è¨ˆå¯ä»¥ï¼š
  * å¦‚æœç”¨ Streamlit åŸ·è¡Œï¼Œå•Ÿå‹•ç¶²é ä»‹é¢
  * å¦‚æœç›´æ¥ç”¨ Python åŸ·è¡Œï¼Œåœ¨çµ‚ç«¯æ©Ÿé¡¯ç¤ºç¯„ä¾‹æ¸¬è©¦çµæœ
* ç›®å‰è¢«è¨»è§£æ‰ï¼Œæ”¹ç‚ºç›´æ¥åŸ·è¡Œ `main()`

---

## ğŸ“ ç¸½çµ

é€™å€‹ç¨‹å¼çš„æ ¸å¿ƒæ¶æ§‹ï¼š

1. **ç’°å¢ƒè¨­å®š** â†’ é¿å… PyTorch èˆ‡ Streamlit è¡çª
2. **æ¨¡å‹è¼‰å…¥** â†’ å¾ Hugging Face ä¸‹è¼‰ä¸¦å¿«å– AI æª¢æ¸¬æ¨¡å‹
3. **é æ¸¬åŠŸèƒ½** â†’ ä½¿ç”¨æ¨¡å‹æˆ–å•Ÿç™¼å¼æ–¹æ³•åˆ†ææ–‡æœ¬
4. **ç¶²é ä»‹é¢** â†’ ç”¨ Streamlit å»ºç«‹äº’å‹•å¼æ‡‰ç”¨ç¨‹å¼
5. **çµæœè¦–è¦ºåŒ–** â†’ æ¸…æ¥šé¡¯ç¤º AI/äººé¡æ©Ÿç‡å’Œè©®é‡‹

é—œéµæŠ€è¡“é»ï¼š

* âœ… ä½¿ç”¨ `os.environ` é å…ˆè§£æ±ºè¡çª
* âœ… æ¢ä»¶å¼åŒ¯å…¥ç¢ºä¿ç¨‹å¼ç¸½èƒ½é‹è¡Œ
* âœ… å¿«å–æ©Ÿåˆ¶é¿å…é‡è¤‡è¼‰å…¥å¤§å‹æ¨¡å‹
* âœ… å¤šç¨®è¼¸å‡ºæ ¼å¼çš„è™•ç†
* âœ… å„ªé›…çš„éŒ¯èª¤è™•ç†å’Œä½¿ç”¨è€…æç¤º
